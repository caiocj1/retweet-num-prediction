{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import gensim, logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "       retweets_count  favorites_count  followers_count  statuses_count  \\\ncount   353969.000000    353969.000000     3.539690e+05    3.539690e+05   \nmean        15.831810        46.655442     2.021548e+04    4.808507e+04   \nstd        241.986723       852.044385     2.598715e+05    1.133854e+05   \nmin          0.000000         0.000000     0.000000e+00    1.000000e+00   \n25%          0.000000         0.000000     1.600000e+02    2.972000e+03   \n50%          1.000000         0.000000     7.260000e+02    1.250100e+04   \n75%          3.000000         1.000000     2.283000e+03    4.352200e+04   \nmax      63674.000000    122591.000000     1.441710e+07    8.183508e+06   \n\n       friends_count       verified     timestamp       TweetID  \ncount  353969.000000  353969.000000  3.539690e+05  3.539690e+05  \nmean     1459.289003       0.030005  1.647004e+12  6.872503e+05  \nstd      2502.933271       0.170602  4.846468e+09  4.175793e+05  \nmin         0.000000       0.000000  1.301178e+12  3.000000e+00  \n25%       214.000000       0.000000  1.647068e+12  3.194490e+05  \n50%       693.000000       0.000000  1.647292e+12  6.719730e+05  \n75%      1804.000000       0.000000  1.647532e+12  1.049644e+06  \nmax    237269.000000       1.000000  1.647727e+12  1.434456e+06  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>retweets_count</th>\n      <th>favorites_count</th>\n      <th>followers_count</th>\n      <th>statuses_count</th>\n      <th>friends_count</th>\n      <th>verified</th>\n      <th>timestamp</th>\n      <th>TweetID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>353969.000000</td>\n      <td>353969.000000</td>\n      <td>3.539690e+05</td>\n      <td>3.539690e+05</td>\n      <td>353969.000000</td>\n      <td>353969.000000</td>\n      <td>3.539690e+05</td>\n      <td>3.539690e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>15.831810</td>\n      <td>46.655442</td>\n      <td>2.021548e+04</td>\n      <td>4.808507e+04</td>\n      <td>1459.289003</td>\n      <td>0.030005</td>\n      <td>1.647004e+12</td>\n      <td>6.872503e+05</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>241.986723</td>\n      <td>852.044385</td>\n      <td>2.598715e+05</td>\n      <td>1.133854e+05</td>\n      <td>2502.933271</td>\n      <td>0.170602</td>\n      <td>4.846468e+09</td>\n      <td>4.175793e+05</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.301178e+12</td>\n      <td>3.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.600000e+02</td>\n      <td>2.972000e+03</td>\n      <td>214.000000</td>\n      <td>0.000000</td>\n      <td>1.647068e+12</td>\n      <td>3.194490e+05</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>7.260000e+02</td>\n      <td>1.250100e+04</td>\n      <td>693.000000</td>\n      <td>0.000000</td>\n      <td>1.647292e+12</td>\n      <td>6.719730e+05</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>2.283000e+03</td>\n      <td>4.352200e+04</td>\n      <td>1804.000000</td>\n      <td>0.000000</td>\n      <td>1.647532e+12</td>\n      <td>1.049644e+06</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>63674.000000</td>\n      <td>122591.000000</td>\n      <td>1.441710e+07</td>\n      <td>8.183508e+06</td>\n      <td>237269.000000</td>\n      <td>1.000000</td>\n      <td>1.647727e+12</td>\n      <td>1.434456e+06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/evaluation.csv')\n",
    "train_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  retweets_count  \\\n0                 rt refarcir macron ans nom prépare               3   \n1                                          populaire               0   \n2                                faut dégager cinglé               3   \n3  enseignants mettre prescriptions président rép...               0   \n4                         mafieuse oppressive macron               0   \n\n   favorites_count  followers_count  statuses_count  friends_count mentions  \\\n0                0             3682          453535           3628       []   \n1                0               86            1016            284       []   \n2                1             1944           28234           1995       []   \n3                0                1            1072              0       []   \n4                0            13957           25311          10841       []   \n\n                          urls  verified hashtags      timestamp  TweetID  \n0                           []         0       []  1646978048000   832509  \n1                           []         0       []  1647694288000  1388011  \n2                           []         0       []  1647370048000    63896  \n3  ['https://t.co/rytlted08g']         0       []  1647256282000   979251  \n4                           []         0       []  1647258374000  1040049  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>retweets_count</th>\n      <th>favorites_count</th>\n      <th>followers_count</th>\n      <th>statuses_count</th>\n      <th>friends_count</th>\n      <th>mentions</th>\n      <th>urls</th>\n      <th>verified</th>\n      <th>hashtags</th>\n      <th>timestamp</th>\n      <th>TweetID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rt refarcir macron ans nom prépare</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3682</td>\n      <td>453535</td>\n      <td>3628</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1646978048000</td>\n      <td>832509</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>populaire</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86</td>\n      <td>1016</td>\n      <td>284</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647694288000</td>\n      <td>1388011</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>faut dégager cinglé</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1944</td>\n      <td>28234</td>\n      <td>1995</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647370048000</td>\n      <td>63896</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>enseignants mettre prescriptions président rép...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1072</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>['https://t.co/rytlted08g']</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647256282000</td>\n      <td>979251</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mafieuse oppressive macron</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13957</td>\n      <td>25311</td>\n      <td>10841</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647258374000</td>\n      <td>1040049</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "   favorites_count  followers_count  statuses_count  friends_count  urls  \\\n0                0             3682          453535           3628     0   \n1                0               86            1016            284     0   \n2                1             1944           28234           1995     0   \n3                0                1            1072              0     1   \n4                0            13957           25311          10841     0   \n\n   verified  hashtags  \n0         0         0  \n1         0         0  \n2         0         0  \n3         0         0  \n4         0         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>favorites_count</th>\n      <th>followers_count</th>\n      <th>statuses_count</th>\n      <th>friends_count</th>\n      <th>urls</th>\n      <th>verified</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3682</td>\n      <td>453535</td>\n      <td>3628</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>86</td>\n      <td>1016</td>\n      <td>284</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1944</td>\n      <td>28234</td>\n      <td>1995</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1072</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>13957</td>\n      <td>25311</td>\n      <td>10841</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_input = train_df.drop(['TweetID', 'timestamp', 'mentions', 'retweets_count', 'text'], axis=1)\n",
    "\n",
    "train_df_input.urls = train_df_input.urls.apply(ast.literal_eval)\n",
    "train_df_input.urls = train_df_input.urls.apply(len)\n",
    "\n",
    "train_df_input.hashtags = train_df_input.hashtags.apply(ast.literal_eval)\n",
    "train_df_input.hashtags = train_df_input.hashtags.apply(len)\n",
    "\n",
    "train_df_input.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "test_df_input = test_df.drop(['TweetID', 'timestamp', 'mentions', 'text'], axis=1)\n",
    "\n",
    "test_df_input.urls = test_df_input.urls.apply(ast.literal_eval)\n",
    "test_df_input.urls = test_df_input.urls.apply(len)\n",
    "\n",
    "test_df_input.hashtags = test_df_input.hashtags.apply(ast.literal_eval)\n",
    "test_df_input.hashtags = test_df_input.hashtags.apply(len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# k-fold Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def kfold_cv(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             n_splits: int = 10,\n",
    "             method: str = 'knn',\n",
    "             n_neighbors: int = 1):\n",
    "    kf = sklearn.model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=123456)\n",
    "    all_splits = [i for i in kf.split(X)]\n",
    "\n",
    "    train_mse = []\n",
    "    val_mse = []\n",
    "\n",
    "    model = None\n",
    "    if method == 'knn':\n",
    "        model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "\n",
    "    for k in range(n_splits):\n",
    "        print('Training split', k)\n",
    "        train_indexes, val_indexes = all_splits[k]\n",
    "\n",
    "        train_X = X.iloc[train_indexes].values\n",
    "        mean = train_X.mean(0)\n",
    "        std = train_X.std(0)\n",
    "        train_X = (train_X - mean) / std\n",
    "        train_y = y.iloc[train_indexes].values\n",
    "\n",
    "        val_X = X.iloc[val_indexes].values\n",
    "        val_X = (val_X - mean) / std\n",
    "        val_y = y.iloc[val_indexes].values\n",
    "\n",
    "        model.fit(train_X, train_y)\n",
    "\n",
    "        train_predictions = model.predict(train_X)\n",
    "        val_predictions = model.predict(val_X)\n",
    "\n",
    "        train_mse.append(np.abs(train_predictions - train_y).mean())\n",
    "        val_mse.append(np.abs(val_predictions - val_y).mean())\n",
    "\n",
    "    return (sum(train_mse) / len(train_mse), sum(val_mse) / len(val_mse))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# for n in range(7, 17, 2):\n",
    "#     print(n, 'neighbors...')\n",
    "#     print(kfold_cv(train_df_input, train_df['retweets_count'], n_splits=5, n_neighbors=n))\n",
    "\n",
    "# Output of above loop\n",
    "knn_mse = { 1: (0.0022840985090, 8.82755207099830),\n",
    "            3: (5.2445855803790, 7.50757052066827),\n",
    "            5: (5.9034562077925, 7.29391667684989),\n",
    "            7: (6.2303256518832, 7.21462506305364),\n",
    "            9: (6.4206208103262, 7.22025110296241),\n",
    "            11: (6.5645450982183, 7.21998145087708),\n",
    "            13: (6.6775331234898, 7.24705572946240),\n",
    "            15: (6.7655345805517, 7.28415542343034)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submit simple models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "make_submission = False\n",
    "\n",
    "if make_submission:\n",
    "    mean = train_df_input.values.mean(0)\n",
    "    std = train_df_input.values.std(0)\n",
    "\n",
    "    train_X = (train_df_input.values - mean) /  std\n",
    "    test_X = (test_df_input.values - mean) / std\n",
    "\n",
    "    test_ids = test_df[['TweetID']]\n",
    "    model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=7)\n",
    "    model.fit(train_X, train_df['retweets_count'])\n",
    "    test_predictions = model.predict(test_X)\n",
    "\n",
    "    submission_df = pd.DataFrame(data={'retweets_count': test_predictions})\n",
    "    submission_df = pd.concat([test_ids, submission_df], axis=1)\n",
    "    submission_df.to_csv('data/submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 12:31:07,859 : INFO : collecting all words and their counts\n",
      "2022-11-14 12:31:08,114 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-11-14 12:31:08,171 : INFO : PROGRESS: at sentence #10000, processed 92512 words, keeping 18699 word types\n",
      "2022-11-14 12:31:08,212 : INFO : PROGRESS: at sentence #20000, processed 184983 words, keeping 27361 word types\n",
      "2022-11-14 12:31:08,254 : INFO : PROGRESS: at sentence #30000, processed 277832 words, keeping 33853 word types\n",
      "2022-11-14 12:31:08,316 : INFO : PROGRESS: at sentence #40000, processed 370444 words, keeping 39273 word types\n",
      "2022-11-14 12:31:08,365 : INFO : PROGRESS: at sentence #50000, processed 462540 words, keeping 43943 word types\n",
      "2022-11-14 12:31:08,443 : INFO : PROGRESS: at sentence #60000, processed 555094 words, keeping 47971 word types\n",
      "2022-11-14 12:31:08,489 : INFO : PROGRESS: at sentence #70000, processed 646885 words, keeping 51526 word types\n",
      "2022-11-14 12:31:08,537 : INFO : PROGRESS: at sentence #80000, processed 740165 words, keeping 55050 word types\n",
      "2022-11-14 12:31:08,583 : INFO : PROGRESS: at sentence #90000, processed 833010 words, keeping 58312 word types\n",
      "2022-11-14 12:31:08,637 : INFO : PROGRESS: at sentence #100000, processed 925776 words, keeping 61385 word types\n",
      "2022-11-14 12:31:08,696 : INFO : PROGRESS: at sentence #110000, processed 1017901 words, keeping 64109 word types\n",
      "2022-11-14 12:31:08,745 : INFO : PROGRESS: at sentence #120000, processed 1110686 words, keeping 66964 word types\n",
      "2022-11-14 12:31:08,805 : INFO : PROGRESS: at sentence #130000, processed 1203836 words, keeping 69643 word types\n",
      "2022-11-14 12:31:08,849 : INFO : PROGRESS: at sentence #140000, processed 1296301 words, keeping 72131 word types\n",
      "2022-11-14 12:31:08,889 : INFO : PROGRESS: at sentence #150000, processed 1387892 words, keeping 74363 word types\n",
      "2022-11-14 12:31:08,942 : INFO : PROGRESS: at sentence #160000, processed 1479938 words, keeping 76577 word types\n",
      "2022-11-14 12:31:08,998 : INFO : PROGRESS: at sentence #170000, processed 1573692 words, keeping 78870 word types\n",
      "2022-11-14 12:31:09,046 : INFO : PROGRESS: at sentence #180000, processed 1666272 words, keeping 80956 word types\n",
      "2022-11-14 12:31:09,088 : INFO : PROGRESS: at sentence #190000, processed 1759039 words, keeping 82971 word types\n",
      "2022-11-14 12:31:09,133 : INFO : PROGRESS: at sentence #200000, processed 1851197 words, keeping 84938 word types\n",
      "2022-11-14 12:31:09,178 : INFO : PROGRESS: at sentence #210000, processed 1943040 words, keeping 86918 word types\n",
      "2022-11-14 12:31:09,233 : INFO : PROGRESS: at sentence #220000, processed 2035371 words, keeping 88754 word types\n",
      "2022-11-14 12:31:09,275 : INFO : PROGRESS: at sentence #230000, processed 2128090 words, keeping 90559 word types\n",
      "2022-11-14 12:31:09,313 : INFO : PROGRESS: at sentence #240000, processed 2220527 words, keeping 92353 word types\n",
      "2022-11-14 12:31:09,360 : INFO : PROGRESS: at sentence #250000, processed 2313153 words, keeping 94023 word types\n",
      "2022-11-14 12:31:09,424 : INFO : PROGRESS: at sentence #260000, processed 2405451 words, keeping 95714 word types\n",
      "2022-11-14 12:31:09,465 : INFO : PROGRESS: at sentence #270000, processed 2498430 words, keeping 97335 word types\n",
      "2022-11-14 12:31:09,505 : INFO : PROGRESS: at sentence #280000, processed 2590195 words, keeping 98822 word types\n",
      "2022-11-14 12:31:09,546 : INFO : PROGRESS: at sentence #290000, processed 2681934 words, keeping 100404 word types\n",
      "2022-11-14 12:31:09,589 : INFO : PROGRESS: at sentence #300000, processed 2774645 words, keeping 102065 word types\n",
      "2022-11-14 12:31:09,632 : INFO : PROGRESS: at sentence #310000, processed 2867600 words, keeping 103538 word types\n",
      "2022-11-14 12:31:09,675 : INFO : PROGRESS: at sentence #320000, processed 2960934 words, keeping 105039 word types\n",
      "2022-11-14 12:31:09,720 : INFO : PROGRESS: at sentence #330000, processed 3052412 words, keeping 106549 word types\n",
      "2022-11-14 12:31:09,770 : INFO : PROGRESS: at sentence #340000, processed 3145245 words, keeping 108025 word types\n",
      "2022-11-14 12:31:09,819 : INFO : PROGRESS: at sentence #350000, processed 3238509 words, keeping 109481 word types\n",
      "2022-11-14 12:31:09,842 : INFO : collected 110060 word types from a corpus of 3275127 raw words and 353969 sentences\n",
      "2022-11-14 12:31:09,844 : INFO : Creating a fresh vocabulary\n",
      "2022-11-14 12:31:10,386 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 110060 unique words (100.00% of original 110060, drops 0)', 'datetime': '2022-11-14T12:31:10.385257', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-11-14 12:31:10,388 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3275127 word corpus (100.00% of original 3275127, drops 0)', 'datetime': '2022-11-14T12:31:10.388259', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-11-14 12:31:11,136 : INFO : deleting the raw counts dictionary of 110060 items\n",
      "2022-11-14 12:31:11,148 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2022-11-14 12:31:11,149 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3027731.485837764 word corpus (92.4%% of prior 3275127)', 'datetime': '2022-11-14T12:31:11.149528', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-11-14 12:31:12,240 : INFO : estimated required memory for 110060 words and 256 dimensions: 280432880 bytes\n",
      "2022-11-14 12:31:12,242 : INFO : resetting layer weights\n",
      "2022-11-14 12:31:12,360 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-11-14T12:31:12.360287', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2022-11-14 12:31:12,361 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 110060 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-11-14T12:31:12.361387', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-11-14 12:31:13,434 : INFO : EPOCH 0 - PROGRESS: at 14.04% examples, 409383 words/s, in_qsize 8, out_qsize 1\n",
      "2022-11-14 12:31:14,448 : INFO : EPOCH 0 - PROGRESS: at 37.50% examples, 553898 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:15,458 : INFO : EPOCH 0 - PROGRESS: at 57.97% examples, 573350 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:16,517 : INFO : EPOCH 0 - PROGRESS: at 78.45% examples, 576299 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:17,549 : INFO : EPOCH 0 - PROGRESS: at 92.50% examples, 543383 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:17,924 : INFO : EPOCH 0: training on 3275127 raw words (3028112 effective words) took 5.5s, 547740 effective words/s\n",
      "2022-11-14 12:31:18,952 : INFO : EPOCH 1 - PROGRESS: at 18.63% examples, 561296 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:19,970 : INFO : EPOCH 1 - PROGRESS: at 35.99% examples, 539382 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:20,977 : INFO : EPOCH 1 - PROGRESS: at 53.99% examples, 539910 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:21,991 : INFO : EPOCH 1 - PROGRESS: at 70.80% examples, 530253 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:22,995 : INFO : EPOCH 1 - PROGRESS: at 89.73% examples, 538310 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:23,508 : INFO : EPOCH 1: training on 3275127 raw words (3027683 effective words) took 5.6s, 544606 effective words/s\n",
      "2022-11-14 12:31:24,550 : INFO : EPOCH 2 - PROGRESS: at 17.69% examples, 529245 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:25,561 : INFO : EPOCH 2 - PROGRESS: at 38.11% examples, 571012 words/s, in_qsize 6, out_qsize 2\n",
      "2022-11-14 12:31:26,561 : INFO : EPOCH 2 - PROGRESS: at 57.04% examples, 571450 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:27,581 : INFO : EPOCH 2 - PROGRESS: at 77.21% examples, 578290 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:28,589 : INFO : EPOCH 2 - PROGRESS: at 93.72% examples, 561720 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:28,911 : INFO : EPOCH 2: training on 3275127 raw words (3028361 effective words) took 5.4s, 563524 effective words/s\n",
      "2022-11-14 12:31:29,984 : INFO : EPOCH 3 - PROGRESS: at 15.87% examples, 454244 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:30,990 : INFO : EPOCH 3 - PROGRESS: at 33.87% examples, 496986 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:31,998 : INFO : EPOCH 3 - PROGRESS: at 53.07% examples, 523289 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:33,028 : INFO : EPOCH 3 - PROGRESS: at 68.96% examples, 509118 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:34,040 : INFO : EPOCH 3 - PROGRESS: at 89.73% examples, 531241 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:34,590 : INFO : EPOCH 3: training on 3275127 raw words (3028028 effective words) took 5.7s, 534570 effective words/s\n",
      "2022-11-14 12:31:35,616 : INFO : EPOCH 4 - PROGRESS: at 18.63% examples, 557523 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:36,641 : INFO : EPOCH 4 - PROGRESS: at 37.50% examples, 558316 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:37,665 : INFO : EPOCH 4 - PROGRESS: at 57.66% examples, 570772 words/s, in_qsize 6, out_qsize 1\n",
      "2022-11-14 12:31:38,682 : INFO : EPOCH 4 - PROGRESS: at 72.64% examples, 539486 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:39,701 : INFO : EPOCH 4 - PROGRESS: at 90.33% examples, 536750 words/s, in_qsize 8, out_qsize 1\n",
      "2022-11-14 12:31:40,193 : INFO : EPOCH 4: training on 3275127 raw words (3027717 effective words) took 5.6s, 541895 effective words/s\n",
      "2022-11-14 12:31:41,221 : INFO : EPOCH 5 - PROGRESS: at 14.65% examples, 441398 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 12:31:42,225 : INFO : EPOCH 5 - PROGRESS: at 34.47% examples, 519662 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:43,252 : INFO : EPOCH 5 - PROGRESS: at 53.37% examples, 532583 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:44,263 : INFO : EPOCH 5 - PROGRESS: at 75.69% examples, 566248 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:45,283 : INFO : EPOCH 5 - PROGRESS: at 97.36% examples, 581753 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:45,382 : INFO : EPOCH 5: training on 3275127 raw words (3027519 effective words) took 5.2s, 586076 effective words/s\n",
      "2022-11-14 12:31:46,408 : INFO : EPOCH 6 - PROGRESS: at 20.16% examples, 600944 words/s, in_qsize 6, out_qsize 1\n",
      "2022-11-14 12:31:47,422 : INFO : EPOCH 6 - PROGRESS: at 39.95% examples, 596731 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:48,436 : INFO : EPOCH 6 - PROGRESS: at 50.33% examples, 501118 words/s, in_qsize 6, out_qsize 0\n",
      "2022-11-14 12:31:49,454 : INFO : EPOCH 6 - PROGRESS: at 64.09% examples, 477787 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:50,455 : INFO : EPOCH 6 - PROGRESS: at 83.04% examples, 496503 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:51,422 : INFO : EPOCH 6: training on 3275127 raw words (3027465 effective words) took 6.0s, 502167 effective words/s\n",
      "2022-11-14 12:31:52,464 : INFO : EPOCH 7 - PROGRESS: at 18.32% examples, 543893 words/s, in_qsize 6, out_qsize 1\n",
      "2022-11-14 12:31:53,471 : INFO : EPOCH 7 - PROGRESS: at 35.07% examples, 524385 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:54,482 : INFO : EPOCH 7 - PROGRESS: at 54.60% examples, 544526 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:55,491 : INFO : EPOCH 7 - PROGRESS: at 73.55% examples, 550301 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:56,502 : INFO : EPOCH 7 - PROGRESS: at 94.03% examples, 562634 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 12:31:56,762 : INFO : EPOCH 7: training on 3275127 raw words (3027357 effective words) took 5.3s, 569380 effective words/s\n",
      "2022-11-14 12:31:56,764 : INFO : Word2Vec lifecycle event {'msg': 'training on 26201016 raw words (24222242 effective words) took 44.4s, 545530 effective words/s', 'datetime': '2022-11-14T12:31:56.764247', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-11-14 12:31:56,765 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=110060, vector_size=256, alpha=0.025>', 'datetime': '2022-11-14T12:31:56.765228', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2022-11-14 12:31:56,805 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'models/word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-11-14T12:31:56.805584', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2022-11-14 12:31:56,806 : INFO : storing np array 'vectors' to models/word2vec.model.wv.vectors.npy\n",
      "2022-11-14 12:31:57,370 : INFO : storing np array 'syn1neg' to models/word2vec.model.syn1neg.npy\n",
      "2022-11-14 12:31:58,922 : INFO : not storing attribute cum_table\n",
      "2022-11-14 12:31:58,998 : INFO : saved models/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, text_df: pd.DataFrame):\n",
    "        self.df = text_df\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tweet in self.df['text'].to_list():\n",
    "            yield tweet.split(' ')\n",
    "\n",
    "tweets = Corpus(train_df)\n",
    "model = gensim.models.Word2Vec(tweets, min_count=1, vector_size=256, workers=4, epochs=8)\n",
    "model.save('models/word2vec.model')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
