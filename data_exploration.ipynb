{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import gensim, logging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "       retweets_count  favorites_count  followers_count  statuses_count  \\\ncount   353969.000000    353969.000000     3.539690e+05    3.539690e+05   \nmean        15.831810        46.655442     2.021548e+04    4.808507e+04   \nstd        241.986723       852.044385     2.598715e+05    1.133854e+05   \nmin          0.000000         0.000000     0.000000e+00    1.000000e+00   \n25%          0.000000         0.000000     1.600000e+02    2.972000e+03   \n50%          1.000000         0.000000     7.260000e+02    1.250100e+04   \n75%          3.000000         1.000000     2.283000e+03    4.352200e+04   \nmax      63674.000000    122591.000000     1.441710e+07    8.183508e+06   \n\n       friends_count       verified     timestamp       TweetID  \ncount  353969.000000  353969.000000  3.539690e+05  3.539690e+05  \nmean     1459.289003       0.030005  1.647004e+12  6.872503e+05  \nstd      2502.933271       0.170602  4.846468e+09  4.175793e+05  \nmin         0.000000       0.000000  1.301178e+12  3.000000e+00  \n25%       214.000000       0.000000  1.647068e+12  3.194490e+05  \n50%       693.000000       0.000000  1.647292e+12  6.719730e+05  \n75%      1804.000000       0.000000  1.647532e+12  1.049644e+06  \nmax    237269.000000       1.000000  1.647727e+12  1.434456e+06  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>retweets_count</th>\n      <th>favorites_count</th>\n      <th>followers_count</th>\n      <th>statuses_count</th>\n      <th>friends_count</th>\n      <th>verified</th>\n      <th>timestamp</th>\n      <th>TweetID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>353969.000000</td>\n      <td>353969.000000</td>\n      <td>3.539690e+05</td>\n      <td>3.539690e+05</td>\n      <td>353969.000000</td>\n      <td>353969.000000</td>\n      <td>3.539690e+05</td>\n      <td>3.539690e+05</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>15.831810</td>\n      <td>46.655442</td>\n      <td>2.021548e+04</td>\n      <td>4.808507e+04</td>\n      <td>1459.289003</td>\n      <td>0.030005</td>\n      <td>1.647004e+12</td>\n      <td>6.872503e+05</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>241.986723</td>\n      <td>852.044385</td>\n      <td>2.598715e+05</td>\n      <td>1.133854e+05</td>\n      <td>2502.933271</td>\n      <td>0.170602</td>\n      <td>4.846468e+09</td>\n      <td>4.175793e+05</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000e+00</td>\n      <td>1.000000e+00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.301178e+12</td>\n      <td>3.000000e+00</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.600000e+02</td>\n      <td>2.972000e+03</td>\n      <td>214.000000</td>\n      <td>0.000000</td>\n      <td>1.647068e+12</td>\n      <td>3.194490e+05</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>7.260000e+02</td>\n      <td>1.250100e+04</td>\n      <td>693.000000</td>\n      <td>0.000000</td>\n      <td>1.647292e+12</td>\n      <td>6.719730e+05</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>3.000000</td>\n      <td>1.000000</td>\n      <td>2.283000e+03</td>\n      <td>4.352200e+04</td>\n      <td>1804.000000</td>\n      <td>0.000000</td>\n      <td>1.647532e+12</td>\n      <td>1.049644e+06</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>63674.000000</td>\n      <td>122591.000000</td>\n      <td>1.441710e+07</td>\n      <td>8.183508e+06</td>\n      <td>237269.000000</td>\n      <td>1.000000</td>\n      <td>1.647727e+12</td>\n      <td>1.434456e+06</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/evaluation.csv')\n",
    "train_df.describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  retweets_count  \\\n0                 rt refarcir macron ans nom prépare               3   \n1                                          populaire               0   \n2                                faut dégager cinglé               3   \n3  enseignants mettre prescriptions président rép...               0   \n4                         mafieuse oppressive macron               0   \n\n   favorites_count  followers_count  statuses_count  friends_count mentions  \\\n0                0             3682          453535           3628       []   \n1                0               86            1016            284       []   \n2                1             1944           28234           1995       []   \n3                0                1            1072              0       []   \n4                0            13957           25311          10841       []   \n\n                          urls  verified hashtags      timestamp  TweetID  \n0                           []         0       []  1646978048000   832509  \n1                           []         0       []  1647694288000  1388011  \n2                           []         0       []  1647370048000    63896  \n3  ['https://t.co/rytlted08g']         0       []  1647256282000   979251  \n4                           []         0       []  1647258374000  1040049  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>retweets_count</th>\n      <th>favorites_count</th>\n      <th>followers_count</th>\n      <th>statuses_count</th>\n      <th>friends_count</th>\n      <th>mentions</th>\n      <th>urls</th>\n      <th>verified</th>\n      <th>hashtags</th>\n      <th>timestamp</th>\n      <th>TweetID</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>rt refarcir macron ans nom prépare</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3682</td>\n      <td>453535</td>\n      <td>3628</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1646978048000</td>\n      <td>832509</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>populaire</td>\n      <td>0</td>\n      <td>0</td>\n      <td>86</td>\n      <td>1016</td>\n      <td>284</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647694288000</td>\n      <td>1388011</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>faut dégager cinglé</td>\n      <td>3</td>\n      <td>1</td>\n      <td>1944</td>\n      <td>28234</td>\n      <td>1995</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647370048000</td>\n      <td>63896</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>enseignants mettre prescriptions président rép...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1072</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>['https://t.co/rytlted08g']</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647256282000</td>\n      <td>979251</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mafieuse oppressive macron</td>\n      <td>0</td>\n      <td>0</td>\n      <td>13957</td>\n      <td>25311</td>\n      <td>10841</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>0</td>\n      <td>[]</td>\n      <td>1647258374000</td>\n      <td>1040049</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "   favorites_count  followers_count  statuses_count  friends_count  urls  \\\n0                0             3682          453535           3628     0   \n1                0               86            1016            284     0   \n2                1             1944           28234           1995     0   \n3                0                1            1072              0     1   \n4                0            13957           25311          10841     0   \n\n   verified  hashtags  \n0         0         0  \n1         0         0  \n2         0         0  \n3         0         0  \n4         0         0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>favorites_count</th>\n      <th>followers_count</th>\n      <th>statuses_count</th>\n      <th>friends_count</th>\n      <th>urls</th>\n      <th>verified</th>\n      <th>hashtags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>3682</td>\n      <td>453535</td>\n      <td>3628</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>86</td>\n      <td>1016</td>\n      <td>284</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1944</td>\n      <td>28234</td>\n      <td>1995</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>1072</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>13957</td>\n      <td>25311</td>\n      <td>10841</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_input = train_df.drop(['TweetID', 'timestamp', 'mentions', 'retweets_count', 'text'], axis=1)\n",
    "\n",
    "train_df_input.urls = train_df_input.urls.apply(ast.literal_eval)\n",
    "train_df_input.urls = train_df_input.urls.apply(len)\n",
    "\n",
    "train_df_input.hashtags = train_df_input.hashtags.apply(ast.literal_eval)\n",
    "train_df_input.hashtags = train_df_input.hashtags.apply(len)\n",
    "\n",
    "train_df_input.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "test_df_input = test_df.drop(['TweetID', 'timestamp', 'mentions', 'text'], axis=1)\n",
    "\n",
    "test_df_input.urls = test_df_input.urls.apply(ast.literal_eval)\n",
    "test_df_input.urls = test_df_input.urls.apply(len)\n",
    "\n",
    "test_df_input.hashtags = test_df_input.hashtags.apply(ast.literal_eval)\n",
    "test_df_input.hashtags = test_df_input.hashtags.apply(len)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# k-fold Cross Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def kfold_cv(X: pd.DataFrame,\n",
    "             y: pd.DataFrame,\n",
    "             n_splits: int = 10,\n",
    "             method: str = 'knn',\n",
    "             n_neighbors: int = 1):\n",
    "    kf = sklearn.model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=123456)\n",
    "    all_splits = [i for i in kf.split(X)]\n",
    "\n",
    "    train_mse = []\n",
    "    val_mse = []\n",
    "\n",
    "    model = None\n",
    "    if method == 'knn':\n",
    "        model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "\n",
    "    for k in range(n_splits):\n",
    "        print('Training split', k)\n",
    "        train_indexes, val_indexes = all_splits[k]\n",
    "\n",
    "        train_X = X.iloc[train_indexes].values\n",
    "        mean = train_X.mean(0)\n",
    "        std = train_X.std(0)\n",
    "        train_X = (train_X - mean) / std\n",
    "        train_y = y.iloc[train_indexes].values\n",
    "\n",
    "        val_X = X.iloc[val_indexes].values\n",
    "        val_X = (val_X - mean) / std\n",
    "        val_y = y.iloc[val_indexes].values\n",
    "\n",
    "        model.fit(train_X, train_y)\n",
    "\n",
    "        train_predictions = model.predict(train_X)\n",
    "        val_predictions = model.predict(val_X)\n",
    "\n",
    "        train_mse.append(np.abs(train_predictions - train_y).mean())\n",
    "        val_mse.append(np.abs(val_predictions - val_y).mean())\n",
    "\n",
    "    return (sum(train_mse) / len(train_mse), sum(val_mse) / len(val_mse))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# for n in range(7, 17, 2):\n",
    "#     print(n, 'neighbors...')\n",
    "#     print(kfold_cv(train_df_input, train_df['retweets_count'], n_splits=5, n_neighbors=n))\n",
    "\n",
    "# Output of above loop\n",
    "knn_mse = { 1: (0.0022840985090, 8.82755207099830),\n",
    "            3: (5.2445855803790, 7.50757052066827),\n",
    "            5: (5.9034562077925, 7.29391667684989),\n",
    "            7: (6.2303256518832, 7.21462506305364),\n",
    "            9: (6.4206208103262, 7.22025110296241),\n",
    "            11: (6.5645450982183, 7.21998145087708),\n",
    "            13: (6.6775331234898, 7.24705572946240),\n",
    "            15: (6.7655345805517, 7.28415542343034)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Submit simple models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "make_submission = False\n",
    "\n",
    "if make_submission:\n",
    "    mean = train_df_input.values.mean(0)\n",
    "    std = train_df_input.values.std(0)\n",
    "\n",
    "    train_X = (train_df_input.values - mean) /  std\n",
    "    test_X = (test_df_input.values - mean) / std\n",
    "\n",
    "    test_ids = test_df[['TweetID']]\n",
    "    model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=7)\n",
    "    model.fit(train_X, train_df['retweets_count'])\n",
    "    test_predictions = model.predict(test_X)\n",
    "\n",
    "    submission_df = pd.DataFrame(data={'retweets_count': test_predictions})\n",
    "    submission_df = pd.concat([test_ids, submission_df], axis=1)\n",
    "    submission_df.to_csv('data/submission.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NLP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-14 10:53:26,965 : INFO : collecting all words and their counts\n",
      "2022-11-14 10:53:26,977 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2022-11-14 10:53:27,012 : INFO : PROGRESS: at sentence #10000, processed 92512 words, keeping 18699 word types\n",
      "2022-11-14 10:53:27,048 : INFO : PROGRESS: at sentence #20000, processed 184983 words, keeping 27361 word types\n",
      "2022-11-14 10:53:27,089 : INFO : PROGRESS: at sentence #30000, processed 277832 words, keeping 33853 word types\n",
      "2022-11-14 10:53:27,129 : INFO : PROGRESS: at sentence #40000, processed 370444 words, keeping 39273 word types\n",
      "2022-11-14 10:53:27,162 : INFO : PROGRESS: at sentence #50000, processed 462540 words, keeping 43943 word types\n",
      "2022-11-14 10:53:27,198 : INFO : PROGRESS: at sentence #60000, processed 555094 words, keeping 47971 word types\n",
      "2022-11-14 10:53:27,232 : INFO : PROGRESS: at sentence #70000, processed 646885 words, keeping 51526 word types\n",
      "2022-11-14 10:53:27,266 : INFO : PROGRESS: at sentence #80000, processed 740165 words, keeping 55050 word types\n",
      "2022-11-14 10:53:27,307 : INFO : PROGRESS: at sentence #90000, processed 833010 words, keeping 58312 word types\n",
      "2022-11-14 10:53:27,350 : INFO : PROGRESS: at sentence #100000, processed 925776 words, keeping 61385 word types\n",
      "2022-11-14 10:53:27,387 : INFO : PROGRESS: at sentence #110000, processed 1017901 words, keeping 64109 word types\n",
      "2022-11-14 10:53:27,423 : INFO : PROGRESS: at sentence #120000, processed 1110686 words, keeping 66964 word types\n",
      "2022-11-14 10:53:27,464 : INFO : PROGRESS: at sentence #130000, processed 1203836 words, keeping 69643 word types\n",
      "2022-11-14 10:53:27,506 : INFO : PROGRESS: at sentence #140000, processed 1296301 words, keeping 72131 word types\n",
      "2022-11-14 10:53:27,548 : INFO : PROGRESS: at sentence #150000, processed 1387892 words, keeping 74363 word types\n",
      "2022-11-14 10:53:27,594 : INFO : PROGRESS: at sentence #160000, processed 1479938 words, keeping 76577 word types\n",
      "2022-11-14 10:53:27,642 : INFO : PROGRESS: at sentence #170000, processed 1573692 words, keeping 78870 word types\n",
      "2022-11-14 10:53:27,682 : INFO : PROGRESS: at sentence #180000, processed 1666272 words, keeping 80956 word types\n",
      "2022-11-14 10:53:27,726 : INFO : PROGRESS: at sentence #190000, processed 1759039 words, keeping 82971 word types\n",
      "2022-11-14 10:53:27,768 : INFO : PROGRESS: at sentence #200000, processed 1851197 words, keeping 84938 word types\n",
      "2022-11-14 10:53:27,810 : INFO : PROGRESS: at sentence #210000, processed 1943040 words, keeping 86918 word types\n",
      "2022-11-14 10:53:27,856 : INFO : PROGRESS: at sentence #220000, processed 2035371 words, keeping 88754 word types\n",
      "2022-11-14 10:53:27,895 : INFO : PROGRESS: at sentence #230000, processed 2128090 words, keeping 90559 word types\n",
      "2022-11-14 10:53:27,931 : INFO : PROGRESS: at sentence #240000, processed 2220527 words, keeping 92353 word types\n",
      "2022-11-14 10:53:27,970 : INFO : PROGRESS: at sentence #250000, processed 2313153 words, keeping 94023 word types\n",
      "2022-11-14 10:53:28,010 : INFO : PROGRESS: at sentence #260000, processed 2405451 words, keeping 95714 word types\n",
      "2022-11-14 10:53:28,058 : INFO : PROGRESS: at sentence #270000, processed 2498430 words, keeping 97335 word types\n",
      "2022-11-14 10:53:28,107 : INFO : PROGRESS: at sentence #280000, processed 2590195 words, keeping 98822 word types\n",
      "2022-11-14 10:53:28,150 : INFO : PROGRESS: at sentence #290000, processed 2681934 words, keeping 100404 word types\n",
      "2022-11-14 10:53:28,191 : INFO : PROGRESS: at sentence #300000, processed 2774645 words, keeping 102065 word types\n",
      "2022-11-14 10:53:28,232 : INFO : PROGRESS: at sentence #310000, processed 2867600 words, keeping 103538 word types\n",
      "2022-11-14 10:53:28,280 : INFO : PROGRESS: at sentence #320000, processed 2960934 words, keeping 105039 word types\n",
      "2022-11-14 10:53:28,315 : INFO : PROGRESS: at sentence #330000, processed 3052412 words, keeping 106549 word types\n",
      "2022-11-14 10:53:28,363 : INFO : PROGRESS: at sentence #340000, processed 3145245 words, keeping 108025 word types\n",
      "2022-11-14 10:53:28,415 : INFO : PROGRESS: at sentence #350000, processed 3238509 words, keeping 109481 word types\n",
      "2022-11-14 10:53:28,438 : INFO : collected 110060 word types from a corpus of 3275127 raw words and 353969 sentences\n",
      "2022-11-14 10:53:28,439 : INFO : Creating a fresh vocabulary\n",
      "2022-11-14 10:53:28,864 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 retains 110060 unique words (100.00% of original 110060, drops 0)', 'datetime': '2022-11-14T10:53:28.864564', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-11-14 10:53:28,865 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=1 leaves 3275127 word corpus (100.00% of original 3275127, drops 0)', 'datetime': '2022-11-14T10:53:28.865564', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-11-14 10:53:29,472 : INFO : deleting the raw counts dictionary of 110060 items\n",
      "2022-11-14 10:53:29,475 : INFO : sample=0.001 downsamples 23 most-common words\n",
      "2022-11-14 10:53:29,476 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 3027731.485837764 word corpus (92.4%% of prior 3275127)', 'datetime': '2022-11-14T10:53:29.476452', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'prepare_vocab'}\n",
      "2022-11-14 10:53:30,466 : INFO : estimated required memory for 110060 words and 256 dimensions: 280432880 bytes\n",
      "2022-11-14 10:53:30,467 : INFO : resetting layer weights\n",
      "2022-11-14 10:53:30,615 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2022-11-14T10:53:30.615499', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'build_vocab'}\n",
      "2022-11-14 10:53:30,617 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 110060 vocabulary and 256 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2022-11-14T10:53:30.617483', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-11-14 10:53:31,663 : INFO : EPOCH 0 - PROGRESS: at 16.79% examples, 505988 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:32,664 : INFO : EPOCH 0 - PROGRESS: at 40.88% examples, 617350 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:33,670 : INFO : EPOCH 0 - PROGRESS: at 61.65% examples, 619902 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:34,737 : INFO : EPOCH 0 - PROGRESS: at 84.57% examples, 627499 words/s, in_qsize 8, out_qsize 1\n",
      "2022-11-14 10:53:35,444 : INFO : EPOCH 0: training on 3275127 raw words (3027894 effective words) took 4.8s, 632776 effective words/s\n",
      "2022-11-14 10:53:36,515 : INFO : EPOCH 1 - PROGRESS: at 21.67% examples, 623463 words/s, in_qsize 5, out_qsize 3\n",
      "2022-11-14 10:53:37,539 : INFO : EPOCH 1 - PROGRESS: at 42.72% examples, 622727 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:38,589 : INFO : EPOCH 1 - PROGRESS: at 61.64% examples, 596912 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:39,619 : INFO : EPOCH 1 - PROGRESS: at 82.74% examples, 602483 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:40,418 : INFO : EPOCH 1: training on 3275127 raw words (3027871 effective words) took 5.0s, 611029 effective words/s\n",
      "2022-11-14 10:53:41,447 : INFO : EPOCH 2 - PROGRESS: at 18.01% examples, 543651 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:42,454 : INFO : EPOCH 2 - PROGRESS: at 38.73% examples, 584030 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:53:43,462 : INFO : EPOCH 2 - PROGRESS: at 61.64% examples, 618604 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:53:44,468 : INFO : EPOCH 2 - PROGRESS: at 82.13% examples, 617724 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:45,208 : INFO : EPOCH 2: training on 3275127 raw words (3027377 effective words) took 4.8s, 635607 effective words/s\n",
      "2022-11-14 10:53:46,245 : INFO : EPOCH 3 - PROGRESS: at 20.46% examples, 611218 words/s, in_qsize 6, out_qsize 2\n",
      "2022-11-14 10:53:47,258 : INFO : EPOCH 3 - PROGRESS: at 42.42% examples, 634148 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:48,273 : INFO : EPOCH 3 - PROGRESS: at 61.64% examples, 613705 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:49,286 : INFO : EPOCH 3 - PROGRESS: at 81.20% examples, 606267 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:53:50,321 : INFO : EPOCH 3 - PROGRESS: at 98.57% examples, 586450 words/s, in_qsize 5, out_qsize 0\n",
      "2022-11-14 10:53:50,368 : INFO : EPOCH 3: training on 3275127 raw words (3027860 effective words) took 5.1s, 589546 effective words/s\n",
      "2022-11-14 10:53:51,437 : INFO : EPOCH 4 - PROGRESS: at 15.56% examples, 450763 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:52,470 : INFO : EPOCH 4 - PROGRESS: at 27.45% examples, 400172 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:53,484 : INFO : EPOCH 4 - PROGRESS: at 47.59% examples, 466250 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:54,487 : INFO : EPOCH 4 - PROGRESS: at 69.57% examples, 514439 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:55,501 : INFO : EPOCH 4 - PROGRESS: at 93.10% examples, 551611 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:55,762 : INFO : EPOCH 4: training on 3275127 raw words (3027654 effective words) took 5.4s, 563889 effective words/s\n",
      "2022-11-14 10:53:56,780 : INFO : EPOCH 5 - PROGRESS: at 18.63% examples, 560423 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:53:57,838 : INFO : EPOCH 5 - PROGRESS: at 39.03% examples, 572933 words/s, in_qsize 8, out_qsize 2\n",
      "2022-11-14 10:53:58,840 : INFO : EPOCH 5 - PROGRESS: at 58.58% examples, 578472 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:53:59,844 : INFO : EPOCH 5 - PROGRESS: at 77.21% examples, 574229 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:00,854 : INFO : EPOCH 5 - PROGRESS: at 93.10% examples, 554709 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:01,190 : INFO : EPOCH 5: training on 3275127 raw words (3027703 effective words) took 5.4s, 558969 effective words/s\n",
      "2022-11-14 10:54:02,216 : INFO : EPOCH 6 - PROGRESS: at 16.18% examples, 488642 words/s, in_qsize 6, out_qsize 1\n",
      "2022-11-14 10:54:03,221 : INFO : EPOCH 6 - PROGRESS: at 35.38% examples, 533969 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:04,222 : INFO : EPOCH 6 - PROGRESS: at 54.91% examples, 552957 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:05,234 : INFO : EPOCH 6 - PROGRESS: at 77.21% examples, 581576 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:06,236 : INFO : EPOCH 6 - PROGRESS: at 96.76% examples, 583275 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:06,372 : INFO : EPOCH 6: training on 3275127 raw words (3027802 effective words) took 5.2s, 587040 effective words/s\n",
      "2022-11-14 10:54:07,392 : INFO : EPOCH 7 - PROGRESS: at 19.24% examples, 577660 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:08,403 : INFO : EPOCH 7 - PROGRESS: at 37.81% examples, 567295 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:09,406 : INFO : EPOCH 7 - PROGRESS: at 57.66% examples, 577664 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:10,407 : INFO : EPOCH 7 - PROGRESS: at 74.77% examples, 562654 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:11,411 : INFO : EPOCH 7 - PROGRESS: at 92.20% examples, 555086 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:11,880 : INFO : EPOCH 7: training on 3275127 raw words (3027222 effective words) took 5.5s, 550817 effective words/s\n",
      "2022-11-14 10:54:12,901 : INFO : EPOCH 8 - PROGRESS: at 14.95% examples, 449951 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:13,909 : INFO : EPOCH 8 - PROGRESS: at 36.29% examples, 545916 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:14,940 : INFO : EPOCH 8 - PROGRESS: at 54.60% examples, 543271 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:15,946 : INFO : EPOCH 8 - PROGRESS: at 75.99% examples, 567873 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:16,959 : INFO : EPOCH 8 - PROGRESS: at 95.24% examples, 569214 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:17,152 : INFO : EPOCH 8: training on 3275127 raw words (3027728 effective words) took 5.3s, 575836 effective words/s\n",
      "2022-11-14 10:54:18,205 : INFO : EPOCH 9 - PROGRESS: at 18.63% examples, 548795 words/s, in_qsize 5, out_qsize 3\n",
      "2022-11-14 10:54:19,232 : INFO : EPOCH 9 - PROGRESS: at 38.42% examples, 566810 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:20,357 : INFO : EPOCH 9 - PROGRESS: at 57.66% examples, 549424 words/s, in_qsize 8, out_qsize 2\n",
      "2022-11-14 10:54:21,367 : INFO : EPOCH 9 - PROGRESS: at 76.90% examples, 555846 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:22,454 : INFO : EPOCH 9 - PROGRESS: at 95.85% examples, 549990 words/s, in_qsize 6, out_qsize 2\n",
      "2022-11-14 10:54:22,669 : INFO : EPOCH 9: training on 3275127 raw words (3027660 effective words) took 5.5s, 551413 effective words/s\n",
      "2022-11-14 10:54:23,701 : INFO : EPOCH 10 - PROGRESS: at 21.06% examples, 633764 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:24,709 : INFO : EPOCH 10 - PROGRESS: at 39.34% examples, 591884 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:25,709 : INFO : EPOCH 10 - PROGRESS: at 61.03% examples, 613099 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:26,716 : INFO : EPOCH 10 - PROGRESS: at 79.68% examples, 599888 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:27,645 : INFO : EPOCH 10: training on 3275127 raw words (3028253 effective words) took 4.9s, 611777 effective words/s\n",
      "2022-11-14 10:54:28,691 : INFO : EPOCH 11 - PROGRESS: at 16.18% examples, 478344 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:29,701 : INFO : EPOCH 11 - PROGRESS: at 33.87% examples, 504347 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:30,703 : INFO : EPOCH 11 - PROGRESS: at 47.29% examples, 471813 words/s, in_qsize 8, out_qsize 1\n",
      "2022-11-14 10:54:31,716 : INFO : EPOCH 11 - PROGRESS: at 64.09% examples, 479270 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:32,738 : INFO : EPOCH 11 - PROGRESS: at 82.13% examples, 490186 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:33,739 : INFO : EPOCH 11 - PROGRESS: at 96.15% examples, 479422 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:33,961 : INFO : EPOCH 11: training on 3275127 raw words (3028125 effective words) took 6.3s, 481170 effective words/s\n",
      "2022-11-14 10:54:34,989 : INFO : EPOCH 12 - PROGRESS: at 18.94% examples, 572014 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:35,990 : INFO : EPOCH 12 - PROGRESS: at 36.29% examples, 549231 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:36,997 : INFO : EPOCH 12 - PROGRESS: at 57.97% examples, 583309 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:38,030 : INFO : EPOCH 12 - PROGRESS: at 76.60% examples, 573660 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:39,048 : INFO : EPOCH 12 - PROGRESS: at 92.50% examples, 553253 words/s, in_qsize 8, out_qsize 1\n",
      "2022-11-14 10:54:39,465 : INFO : EPOCH 12: training on 3275127 raw words (3028188 effective words) took 5.5s, 552814 effective words/s\n",
      "2022-11-14 10:54:40,509 : INFO : EPOCH 13 - PROGRESS: at 13.73% examples, 408724 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:41,538 : INFO : EPOCH 13 - PROGRESS: at 32.97% examples, 487552 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:42,560 : INFO : EPOCH 13 - PROGRESS: at 49.72% examples, 490728 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:43,571 : INFO : EPOCH 13 - PROGRESS: at 68.96% examples, 511907 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:44,607 : INFO : EPOCH 13 - PROGRESS: at 88.82% examples, 525587 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:45,141 : INFO : EPOCH 13: training on 3275127 raw words (3028114 effective words) took 5.6s, 535978 effective words/s\n",
      "2022-11-14 10:54:46,175 : INFO : EPOCH 14 - PROGRESS: at 16.79% examples, 506077 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:47,175 : INFO : EPOCH 14 - PROGRESS: at 34.77% examples, 525360 words/s, in_qsize 8, out_qsize 1\n",
      "2022-11-14 10:54:48,210 : INFO : EPOCH 14 - PROGRESS: at 52.77% examples, 525908 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:49,224 : INFO : EPOCH 14 - PROGRESS: at 72.64% examples, 542455 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:50,248 : INFO : EPOCH 14 - PROGRESS: at 90.33% examples, 538706 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:50,753 : INFO : EPOCH 14: training on 3275127 raw words (3027984 effective words) took 5.6s, 542436 effective words/s\n",
      "2022-11-14 10:54:51,776 : INFO : EPOCH 15 - PROGRESS: at 17.69% examples, 530155 words/s, in_qsize 8, out_qsize 0\n",
      "2022-11-14 10:54:52,782 : INFO : EPOCH 15 - PROGRESS: at 37.20% examples, 559122 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:53,795 : INFO : EPOCH 15 - PROGRESS: at 60.42% examples, 604025 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:54,835 : INFO : EPOCH 15 - PROGRESS: at 78.45% examples, 583624 words/s, in_qsize 7, out_qsize 0\n",
      "2022-11-14 10:54:55,779 : INFO : EPOCH 15: training on 3275127 raw words (3027913 effective words) took 5.0s, 603928 effective words/s\n",
      "2022-11-14 10:54:55,781 : INFO : Word2Vec lifecycle event {'msg': 'training on 52402032 raw words (48445348 effective words) took 85.2s, 568852 effective words/s', 'datetime': '2022-11-14T10:54:55.781746', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'train'}\n",
      "2022-11-14 10:54:55,781 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec<vocab=110060, vector_size=256, alpha=0.025>', 'datetime': '2022-11-14T10:54:55.781746', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'created'}\n",
      "2022-11-14 10:54:55,782 : INFO : Word2Vec lifecycle event {'fname_or_handle': 'models/word2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-11-14T10:54:55.782737', 'gensim': '4.2.0', 'python': '3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22000-SP0', 'event': 'saving'}\n",
      "2022-11-14 10:54:55,783 : INFO : storing np array 'vectors' to models/word2vec.model.wv.vectors.npy\n",
      "2022-11-14 10:54:55,958 : INFO : storing np array 'syn1neg' to models/word2vec.model.syn1neg.npy\n",
      "2022-11-14 10:54:56,124 : INFO : not storing attribute cum_table\n",
      "2022-11-14 10:54:56,174 : INFO : saved models/word2vec.model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, text_df: pd.DataFrame):\n",
    "        self.df = text_df\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tweet in self.df['text'].to_list():\n",
    "            yield tweet.split(' ')\n",
    "\n",
    "tweets = Corpus(train_df)\n",
    "model = gensim.models.Word2Vec(tweets, min_count=1, vector_size=256, workers=4, epochs=16)\n",
    "model.save('models/word2vec.model')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
